===============================================================================
CHANGES FROM INITIAL VERSION - READ THIS FIRST
===============================================================================

This is the CORRECTED version of the diazotroph classification pipeline.
The following critical issues from the initial run have been fixed:

===============================================================================
1. GENOME FILTERING - FIXED
===============================================================================

ISSUE: Initial version included both "Complete Genome" AND "Chromosome" 
       assembly levels (937 genomes total)

YOUR REQUIREMENT: Only "Complete Genome" assembly level

FIX: Changed filtering in 01_prepare_data.py to use ONLY:
     df['assembly_level'] == 'Complete Genome'
     
RESULT: Now correctly filters to ~478 genomes (not 937)

===============================================================================
2. PROTEIN DOWNLOAD FAILURES - FIXED
===============================================================================

ISSUE: All 937 downloads failed with "Failed (download error)"
       The compute node couldn't access NCBI servers

ROOT CAUSE: HPC compute nodes typically have restricted network access

FIXES IMPLEMENTED:
A. Enhanced 02_download_proteins.py with:
   - Multiple download methods (datasets CLI, wget, curl, etc.)
   - Better error handling
   - Informative error messages
   - Creates MANUAL_DOWNLOAD.txt if all methods fail

B. Created DOWNLOAD_TROUBLESHOOTING.txt with solutions:
   - Run from login node (MOST COMMON SOLUTION)
   - Use data transfer nodes
   - Manual download methods
   - Batch download scripts
   - Continuation with partial data

RECOMMENDED ACTION:
After running 01_prepare_data.py, SSH to your HPC login node and run:
  python3 02_download_proteins.py

This will likely succeed because login nodes have internet access.

===============================================================================
3. HANDLING MISSING PROTEINS - FIXED
===============================================================================

ISSUE: Pipeline crashed in Step 3 because no proteins were downloaded

FIX: Enhanced all downstream scripts to:
   - Check for protein availability
   - Filter to genomes with downloaded proteins
   - Adjust parameters based on available data
   - Provide informative warnings
   - Allow continuation with partial data

SCRIPTS UPDATED:
- 03_create_gene_families.py: Now counts available proteins, adjusts min_genomes
- 04_classify.py: Uses filtered genome list, adjusts CV folds
- 05_analyze_feature_directionality.py: Uses filtered genome list

===============================================================================
4. PARAMETER ADJUSTMENTS - ADDED
===============================================================================

NEW ADAPTIVE BEHAVIOR:
- Minimum genomes per family: Now 10% of available genomes (min 10, max 40)
- CV folds: Adjusted based on number of genera available
- All scripts check for minimum data requirements
- Warnings provided if data is insufficient

MINIMUM REQUIREMENTS:
- At least 50 genomes for pipeline to run
- At least 100 genomes recommended for meaningful results
- At least 5 genera for genus-level CV

===============================================================================
5. NEW FILES ADDED
===============================================================================

DOWNLOAD_TROUBLESHOOTING.txt
  - Comprehensive guide for fixing download issues
  - Multiple alternative download methods
  - Step-by-step instructions
  - Verification commands

CHANGES.txt (this file)
  - Documents all fixes and changes
  - Helps you understand what went wrong and what's fixed

===============================================================================
6. DOCUMENTATION UPDATES
===============================================================================

All documentation updated to reflect:
- Correct genome count (~478 not 937)
- Expected download issues
- Solutions for common problems
- Adjusted runtime expectations
- More realistic performance expectations

Files updated:
- README.txt: Added troubleshooting section, corrected numbers
- QUICKSTART.txt: Added download notes, corrected expectations
- run_pipeline.sh: No changes needed (already correct)

===============================================================================
HOW TO PROCEED
===============================================================================

OPTION A: Re-run Complete Pipeline
1. Extract this new version
2. Upload to HPC
3. Submit job: sbatch run_pipeline.sh
4. If downloads fail, follow instructions in output or see next option

OPTION B: Manual Download Then Continue (RECOMMENDED)
1. Run Step 1 from compute node or login node:
   python3 01_prepare_data.py nif_hdk_hits_enriched_with_quality_checkm.csv

2. Run Step 2 from LOGIN NODE (not compute node):
   # SSH to login node
   cd ~/diazotroph_classification
   python3 02_download_proteins.py
   
3. Continue with Steps 3-5:
   python3 03_create_gene_families.py 16
   python3 04_classify.py
   python3 05_analyze_feature_directionality.py

OPTION C: Submit as Separate Jobs
Submit Step 2 as a job on login node, rest on compute nodes

===============================================================================
EXPECTED OUTCOMES WITH FIXES
===============================================================================

With ~478 genomes and successful downloads:
- Diazotrophs: ~200 genomes
- Non-diazotrophs: ~278 genomes  
- Gene families: 500-2000 (after filtering)
- Classification accuracy: 75-90% (genus-level CV is conservative)
- Clear feature separation

With partial downloads (200+ genomes):
- Still viable for analysis
- Adjusted parameters will accommodate
- May have fewer gene families
- Performance metrics still meaningful

===============================================================================
QUESTIONS?
===============================================================================

If you still have issues:
1. Check DOWNLOAD_TROUBLESHOOTING.txt first
2. Verify you're running downloads from login node
3. Check protein_sequences/ directory for .faa files
4. Look at the script outputs for specific error messages
5. The pipeline is now designed to work with partial data

===============================================================================
